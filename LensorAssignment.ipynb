{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738a56a-aa9a-4946-889a-76a0ebb5200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def count_nan(tensor):\n",
    "    return torch.sum(torch.isnan(tensor)).item()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_boxes = max(len(item[1]['boxes']) for item in batch)\n",
    "    images = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    nan_count = 0  # Initialize the NaN count\n",
    "\n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "        \n",
    "        num_boxes = len(target['boxes'])\n",
    "# Pad boxes to the maximum number of boxes in the batch\n",
    "        padded_boxes = torch.cat([target['boxes'], torch.zeros(max_boxes - num_boxes, 4)], dim=0)\n",
    "# Count NaN values in boxes\n",
    "        nan_count += count_nan(padded_boxes)\n",
    "        \n",
    "# Pad labels to match the max number of boxes\n",
    "        padded_labels = torch.cat([target['labels'], torch.zeros(max_boxes - len(target['labels']), dtype=torch.int64)], dim=0)\n",
    "# Count NaN values in labels\n",
    "        nan_count += count_nan(padded_labels)\n",
    "        \n",
    "        boxes.append(padded_boxes)\n",
    "        labels.append(padded_labels)\n",
    "    \n",
    "    # Stack images, boxes, and labels tensors\n",
    "    images = torch.stack(images, 0)  # Stack images\n",
    "    boxes = torch.stack(boxes, 0)    # Stack boxes\n",
    "    labels = torch.stack(labels, 0)  # Stack labels\n",
    "    \n",
    "    # Count NaN values in images\n",
    "    nan_count += count_nan(images)\n",
    "    \n",
    "    return images, {\"boxes\": boxes, \"labels\": labels}, nan_count\n",
    "\n",
    "class CustomCocoDetection(CocoDetection):\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "        \n",
    "# Print the target to inspect its format\n",
    "        print(f\"Target at index {idx}: {target}\")\n",
    "        \n",
    "# Ensure target is not empty\n",
    "        if not target or len(target) == 0:\n",
    "            target = {\"boxes\": torch.empty((0, 4), dtype=torch.float32), \"labels\": torch.empty((0,), dtype=torch.int64)}\n",
    "        else:\n",
    "# Convert the bbox from [x, y, width, height] to [x_min, y_min, x_max, y_max]\n",
    "            boxes = [obj[\"bbox\"] for obj in target]\n",
    "            boxes = torch.tensor(\n",
    "                [[x, y, x + w, y + h] for x, y, w, h in boxes if w > 0 and h > 0], dtype=torch.float32\n",
    "            )\n",
    "\n",
    "# Add labels\n",
    "            labels = torch.tensor([obj[\"category_id\"] for obj in target], dtype=torch.int64)\n",
    "\n",
    "# Handle empty boxes\n",
    "            if len(boxes) == 0:\n",
    "                boxes = torch.empty((0, 4), dtype=torch.float32)\n",
    "                labels = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "            target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "# Apply transformations if any\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# Custom transform\n",
    "class CustomTransform:\n",
    "    def __call__(self, img, target):\n",
    "# Ensure image is in a valid tensor format\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            if img.ndimension() == 3:  # Tensor format (C, H, W)\n",
    "                channels = img.shape[0]\n",
    "                # If there are more than 4 channels, it's an invalid image\n",
    "                if channels > 4:  \n",
    "                    raise ValueError(f\"Invalid number of channels: {channels}, expected 1 (grayscale) or 3 (RGB) channels.\")\n",
    "# If there are 3 channels, assume RGB and use ToPILImage\n",
    "                if channels == 3:\n",
    "                    img = transforms.ToPILImage()(img)\n",
    "# If there's 1 channel, assume grayscale and convert to PIL\n",
    "                elif channels == 1:\n",
    "                    img = transforms.ToPILImage()(img)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid image dimensions: {img.shape}, expected (C, H, W) format.\")\n",
    "\n",
    "# Apply ToTensor to the PIL image\n",
    "        img = transforms.ToTensor()(img)  # Apply ToTensor to PIL image\n",
    "        return img, target\n",
    "\n",
    "# paths:\n",
    "train_root = '/Users/lilapetri/Documents/assignment Lensor/vehicle_damage_detection_dataset/images/train'\n",
    "train_annotations = '/Users/lilapetri/Documents/assignment Lensor/vehicle_damage_detection_dataset/annotations/instances_train.json'\n",
    "\n",
    "val_root = '/Users/lilapetri/Documents/assignment Lensor/vehicle_damage_detection_dataset/images/val'\n",
    "val_annotations = '/Users/lilapetri/Documents/assignment Lensor/vehicle_damage_detection_dataset/annotations/instances_val.json'\n",
    "\n",
    "test_root = '/Users/lilapetri/Documents/assignment Lensor/vehicle_damage_detection_dataset/images/test'\n",
    "test_annotations = '/Users/lilapetri/Documents/assignment Lensor/vehicle_damage_detection_dataset/annotations/instances_test.json'\n",
    "\n",
    "transform = CustomTransform()\n",
    "\n",
    "train_dataset = CustomCocoDetection(root=train_root, annFile=train_annotations, transforms=transform)\n",
    "val_dataset = CustomCocoDetection(root=val_root, annFile=val_annotations, transforms=transform)\n",
    "test_dataset = CustomCocoDetection(root=test_root, annFile=test_annotations, transforms=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "total_nan_count = 0\n",
    "for epoch in range(1):  # Example: 10 epochs\n",
    "    running_loss = 0\n",
    "    for images, targets, nan_count in train_loader:\n",
    "# Add NaN count from the current batch\n",
    "        total_nan_count += nan_count \n",
    "\n",
    "# Convert images to a list of tensors\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = [img.to(device) for img in torch.unbind(images, dim=0)]\n",
    "        else:\n",
    "            print(f\"Unexpected images format: {images}\")\n",
    "            continue\n",
    "\n",
    "# Fix target format by unbatching boxes and labels\n",
    "        if isinstance(targets, dict):\n",
    "            if \"boxes\" in targets and \"labels\" in targets:\n",
    "                boxes = torch.unbind(targets[\"boxes\"], dim=0)  # List of [N, 4]\n",
    "                labels = torch.unbind(targets[\"labels\"], dim=0)  # List of [N]\n",
    "                targets = [{\"boxes\": b, \"labels\": l} for b, l in zip(boxes, labels)]\n",
    "            else:\n",
    "                print(f\"Unexpected targets format: {targets}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Unexpected targets format: {targets}\")\n",
    "            continue\n",
    "\n",
    "# Validate and clean bounding boxes\n",
    "        filtered_targets = []\n",
    "        for target in targets:\n",
    "            if \"boxes\" in target and \"labels\" in target:\n",
    "                boxes = target[\"boxes\"]\n",
    "                labels = target[\"labels\"]\n",
    "\n",
    "# Filter invalid boxes\n",
    "                valid_indices = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "                if valid_indices.sum() == 0:  # Skip if no valid boxes\n",
    "                    print(f\"No valid boxes in target: {target}\")\n",
    "                    continue\n",
    "\n",
    "                target[\"boxes\"] = boxes[valid_indices]\n",
    "                target[\"labels\"] = labels[valid_indices]\n",
    "                filtered_targets.append(target)\n",
    "            else:\n",
    "                print(f\"Skipping target with missing keys: {target}\")\n",
    "        \n",
    "# Handle not valid targets\n",
    "        if len(filtered_targets) == 0:\n",
    "            print(\"Skipping batch with no valid targets.\")\n",
    "            continue\n",
    "\n",
    "        targets = filtered_targets\n",
    "\n",
    "# Move target tensors to device\n",
    "        targets = [\n",
    "            {k: v.to(device) for k, v in t.items() if isinstance(v, torch.Tensor)} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += losses.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            continue\n",
    "    print(f\"Epoch {epoch + 1} completed with loss: {running_loss/len(train_loader)}\")\n",
    "    print(f\"NaN values detected in epoch {epoch + 1}: {total_nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d7cd9-1485-43ac-ac6b-eb6b527d0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "batch_size = 32  # Choose an appropriate batch size based on your GPU memory\n",
    "\n",
    "# Initialize counters and storage\n",
    "total_predictions = 0\n",
    "max_predictions = 500\n",
    "y_true = []  # Ground truth labels\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "# Helper function to calculate IoU\n",
    "def calculate_iou(box1, box2):\n",
    "    # Compute intersection\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    if x2_inter <= x1_inter or y2_inter <= y1_inter:\n",
    "        return 0.0  # No overlap\n",
    "    \n",
    "    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
    "    \n",
    "    # Compute union\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "# Iterate over the dataset in batches\n",
    "for i in range(0, len(train_dataset), batch_size):\n",
    "    if total_predictions >= max_predictions:\n",
    "        break  # Stop processing if the maximum number of predictions is reached\n",
    "    \n",
    "    batch_images = []\n",
    "    batch_targets = []\n",
    "    \n",
    "    # Collect the next batch\n",
    "    for j in range(i, min(i + batch_size, len(train_dataset))):\n",
    "        sample_image, sample_target = train_dataset[j]\n",
    "        batch_images.append(sample_image.to(device))\n",
    "        batch_targets.append(sample_target)\n",
    "    \n",
    "    # Stack images into a batch\n",
    "    batch_images = torch.stack(batch_images)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(batch_images)\n",
    "    \n",
    "    # Count the number of predictions in this batch\n",
    "    total_predictions += len(predictions)\n",
    "    \n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        # Skip if no predictions or actuals for this image\n",
    "        if len(prediction['boxes']) == 0 or len(batch_targets[idx]['boxes']) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Average prediction\n",
    "        avg_pred_box = torch.mean(prediction['boxes'], dim=0).round().int().tolist()\n",
    "        avg_pred_label = prediction['labels'].mode().values.item()\n",
    "        \n",
    "        # Average actuals\n",
    "        avg_actual_box = torch.mean(batch_targets[idx]['boxes'], dim=0).round().int().tolist()\n",
    "        avg_actual_label = batch_targets[idx]['labels'].mode().values.item()\n",
    "        \n",
    "        # Append to results\n",
    "        y_pred.append(avg_pred_label)\n",
    "        y_true.append(avg_actual_label)\n",
    "        \n",
    "        # Calculate IoU (optional, for bounding box evaluation)\n",
    "        iou = calculate_iou(avg_pred_box, avg_actual_box)\n",
    "        print(f\"Image {i + idx}: IoU={iou:.2f}, Pred Label={avg_pred_label}, Actual Label={avg_actual_label}\")\n",
    "        \n",
    "        # Visualization (optional)\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(batch_images[idx].permute(1, 2, 0).cpu().numpy())\n",
    "        \n",
    "        # Draw predicted bounding box\n",
    "        rect_pred = patches.Rectangle(\n",
    "            (avg_pred_box[0], avg_pred_box[1]),\n",
    "            avg_pred_box[2] - avg_pred_box[0],\n",
    "            avg_pred_box[3] - avg_pred_box[1],\n",
    "            linewidth=2,\n",
    "            edgecolor='r',\n",
    "            facecolor='none',\n",
    "            label='Prediction'\n",
    "        )\n",
    "        ax.add_patch(rect_pred)\n",
    "        \n",
    "        # Draw actual bounding box\n",
    "        rect_actual = patches.Rectangle(\n",
    "            (avg_actual_box[0], avg_actual_box[1]),\n",
    "            avg_actual_box[2] - avg_actual_box[0],\n",
    "            avg_actual_box[3] - avg_actual_box[1],\n",
    "            linewidth=2,\n",
    "            edgecolor='g',\n",
    "            facecolor='none',\n",
    "            label='Actual'\n",
    "        )\n",
    "        ax.add_patch(rect_actual)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3507da-54e3-4d8b-b122-c1acd72e4230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
